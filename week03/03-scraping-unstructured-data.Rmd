---
title: "Scraping unstructured data"
author: "Pablo Barbera, Akitaka Matsuo, and Martin Lukac"
date: "15 October 2020"
output: html_document
---

### Scraping web data in unstructured format

Loading packages

```{r}
library(tidyverse)
library(stringi)
library(rvest)
library(DT)
```

A common scenario for web scraping is when the data we want is available in plain html, but in different parts of the web, and not in a table format. In this scenario, we will need to find a way to extract each element, and then put it together into a data frame manually.

The motivating example here will be the website [http://ipaidabribe.com](http://ipaidabribe.com), which contains a database of self-reports of bribes in India. We want to learn how much people were asked to pay for different services, and by which departments.

```{r}
url <- 'http://ipaidabribe.com/reports/paid'
```

We will also be using `rvest`, but in a slightly different way: Prior to scraping, we need to identify the CSS selector of each element we want to extract. 

Now, go to the ipaidabribe website (this link: http://ipaidabribe.com/reports/paid) and open the `Inspect Element` section. Then, find the element you want to extract, and copy and paste the CSS selector into R.

Now we're ready to scrape the website:

```{r}
# Reading the HTML code
bribe_html <- read_html(url)

# Identify the CSS selector and add it to the css argument
bribe_nodes <- html_nodes(bribe_html, css = ".paid-amount span")

# Content of CSS selector
bribe_nodes
```

We still need to do some cleaning before the data is usable:

```{r}
# Extract the content of the CSS selector as text
bribe_amount <- html_text(bribe_nodes) %>%
  stri_replace_first_fixed("Paid INR ", "") %>%
  stri_replace_first_regex("\\r.+", "") %>%
  stri_replace_all_fixed(",", "") %>%
  as.numeric

bribe_amount
```

Small note on `stri_replace()`: `first` means to look for the first occurrence, `all` means to apply the rule to all occurrences, and `last` means last occurrence of the searched string; `fixed` is used for searching for fixed string: e.g. `Paid INR`, while `regex` is used for [regular expressions](https://en.wikipedia.org/wiki/Regular_expression). See a cheat sheet [here](https://www.rexegg.com/regex-quickstart.html)

Let's do another one: transactions during which the bribe occurred:

```{r}
transaction <- html_nodes(bribe_html,".transaction a") %>% 
  html_text

transaction
```

And one more: the department that is responsible for these transactions

```{r}
# and one more
dept <- html_nodes(bribe_html, ".department .name a") %>% 
  html_text

dept
```

This was just for one page, but note that there are many pages. How do we scrape the rest? First, following the best practices on coding, we will write a function that takes the URL of each page, scrapes it, and returns the information we want.

```{r}
scrape_bribe <- function(url){
  # Can you combine all previoius sections into one function?
  
  # 1. Read the html
  c_html <- read_html(url)
  
  # 2. Take the html and get the bribe amount
  amount <- c_html %>% 
    html_nodes(".paid-amount span") %>%
    # extract it as text
    html_text%>% 
    stri_replace_first_fixed("Paid INR ", "") %>%
    stri_replace_all_regex("\\r.+", "") %>%
    stri_replace_all_regex(",", "") %>%
    as.numeric()
  
  # 3. Take the html and get the transaction text
  transaction <- html_nodes(c_html, ".transaction a") %>% 
    html_text
  
  # 4. Take the html and get the department text
  dept <- html_nodes(c_html, ".department .name a") %>% 
    html_text
  
  # 5. Combine amount, transaction, and department into one dataframe
  df <- data.frame(amount, transaction, dept, stringsAsFactors = FALSE)
  
  return(df)
}

scrape_bribe(url)
```

And we will start a list of data frames, and put the data frame for the initial page in the first position of that list.

```{r}
data_list <- list(scrape_bribe(url))
```

How should we go about the following pages? Note that the following urls had `page=x`, where `x` is 10, 20, 30... So we will create a base url and then add these additional numbers. (Note that for this exercise we will only scrape the first 5 pages.)

```{r}
base_url <- "http://ipaidabribe.com/reports/paid?page="
pages <- seq(0, 40, by = 10)
```

And now we just need to loop over pages, and use the function we created earlier to scrape the information, and add it to the list. Note that we're adding a couple of seconds between HTTP requests to avoid overloading the page, as well as a message that will informs us of the progress of the loop.

```{r}
for (i in 2:length(pages)){
  # informative message about progress of loop
  message(i, '/', length(pages))
  # prepare URL
  url <- paste0(base_url, pages[i])
  # scrape website
  data_list[[i]] <- scrape_bribe(url)
  # wait a couple of seconds between URL calls
  Sys.sleep(2)
}
```

The final step is to convert the list of data frames into a single data frame that we can work with, using the function `do.call(rbind, LIST)` (where `LIST` is a list of data frames).

```{r}
## Instead of do.call, we will use bind_rows in dplyr which is much faster 
data_all <- bind_rows(data_list)

## Check
View(data_all)
```

Let's get some quick descriptive statistics to check everything worked. First, what is the most common transaction during which a bribe was paid?

```{r}
# frequency table
table(data_all$transaction)

```

What was the average bribe payment?

```{r}
mean(data_all$amount)
median(data_all$amount)

# Can you create a histogram of the amounts paid?
ggplot(data_all, aes(x = _________)) + 
  geom_histogram() + 
  scale_x_log10()

```

And what was the average payment for each department? 

```{r}
agg <- data_all %>%
  group_by(dept) %>% # group the data by department
  summarize(mean_bribe = mean(amount) %>% round(1)) %>% # get the summery statistics
  arrange(-mean_bribe) # order the data by mean bribe amount from largest to smallest

## DT will provide a sortable table in knitted document.
datatable(agg)
```
